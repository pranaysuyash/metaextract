#!/usr/bin/env python3
"""Machine Learning and AI Format Metadata Field Inventory for MetaExtract.

Covers ONNX, TensorFlow SavedModel, PyTorch, TensorRT, OpenVINO, and model metadata.
Reference: ONNX Runtime, TensorFlow, PyTorch, NVIDIA TensorRT, Intel OpenVINO specifications.
"""

from datetime import datetime, timezone
import json
from pathlib import Path


def generate_ml_ai_inventory():
    """Generate comprehensive ML/AI format metadata field inventory."""

    INVENTORY = {
        "generated_at": datetime.now(timezone.utc).isoformat(timespec="seconds"),
        "source": "ONNX, TensorFlow, PyTorch, TensorRT, OpenVINO, Open Neural Network Exchange",
        "description": "Machine learning and AI format metadata including model architectures, tensors, operators, and training parameters",
        "categories": {
            "onnx_metadata": {
                "description": "ONNX model and graph metadata fields",
                "reference": "ONNX Runtime, Open Neural Network Exchange Specification",
                "fields": [
                    "onnx.ir_version",
                    "onnx.opset_import",
                    "onnx.producer_name",
                    "onnx.producer_version",
                    "onnx.domain",
                    "onnx.model_version",
                    "onnx.doc_string",
                    "onnx.graph.name",
                    "onnx.graph.input.name",
                    "onnx.graph.input.type.tensor_type.elem_type",
                    "onnx.graph.input.type.tensor_type.shape.dim.dim_value",
                    "onnx.graph.output.name",
                    "onnx.graph.output.type.tensor_type.elem_type",
                    "onnx.graph.output.type.tensor_type.shape.dim.dim_value",
                    "onnx.graph.node.op_type",
                    "onnx.graph.node.domain",
                    "onnx.graph.node.attribute.name",
                    "onnx.graph.node.attribute.type",
                    "onnx.graph.node.attribute.t",
                    "onnx.graph.node.attribute.f",
                    "onnx.graph.node.attribute.i",
                    "onnx.graph.node.attribute.floats",
                    "onnx.graph.node.attribute.ints",
                    "onnx.graph.initializer.name",
                    "onnx.graph.initializer.data_type",
                    "onnx.graph.initializer.dims",
                    "onnx.graph.value_info.name",
                    "onnx.graph.value_info.type",
                    "onnx.sparse_initializer.name",
                    "onnx.sparse_initializer.indices",
                    "onnx.sparse_initializer.values",
                    "onnx.metadata_props.key",
                    "onnx.metadata_props.value",
                    "onnx.training_info.algorithm",
                    "onnx.training_info.alphas",
                    "onnx.training_info.betas",
                    "onnx.training_info.learning_rate_input",
                    "onnx.function.name",
                    "onnx.function.input",
                    "onnx.function.output",
                    "onnx.function.node",
                    "onnx.function.attribute",
                    "onnx.function.attribute_proto",
                ],
                "count": 42,
            },
            "onnx_operators": {
                "description": "ONNX operator types and attributes",
                "reference": "ONNX Operator Schemas",
                "fields": [
                    "onnx.op.Abs",
                    "onnx.op.Acos",
                    "onnx.op.Acosh",
                    "onnx.op.Add",
                    "onnx.op.And",
                    "onnx.op.ArgMax",
                    "onnx.op.ArgMin",
                    "onnx.op.Asin",
                    "onnx.op.Asinh",
                    "onnx.op.Atan",
                    "onnx.op.Atanh",
                    "onnx.op.Attention",
                    "onnx.op.AveragePool",
                    "onnx.op.BatchNormalization",
                    "onnx.op.Bernoulli",
                    "onnx.op.BitShift",
                    "onnx.op.Cast",
                    "onnx.op.CastLike",
                    "onnx.op.Ceil",
                    "onnx.op.Clip",
                    "onnx.op.Compress",
                    "onnx.op.Concat",
                    "onnx.op.ConcatFromSequence",
                    "onnx.op.Constant",
                    "onnx.op.ConstantOfShape",
                    "onnx.op.Conv",
                    "onnx.op.ConvInteger",
                    "onnx.op.ConvTranspose",
                    "onnx.op.Cos",
                    "onnx.op.Cosh",
                    "onnx.op.CumSum",
                    "onnx.op.DepthToSpace",
                    "onnx.op.DequantizeLinear",
                    "onnx.op.Det",
                    "onnx.op.Div",
                    "onnx.op.Dropout",
                    "onnx.op.Einsum",
                    "onnx.op.Elu",
                    "onnx.op.Equal",
                    "onnx.op.Erf",
                    "onnx.op.Exp",
                    "onnx.op.Expand",
                    "onnx.op.EyeLike",
                    "onnx.op.Flatten",
                    "onnx.op.Floor",
                    "onnx.op.Gather",
                    "onnx.op.GatherElements",
                    "onnx.op.GatherND",
                    "onnx.op.Gemm",
                    "onnx.op.GlobalAveragePool",
                    "onnx.op.GlobalMaxPool",
                    "onnx.op.Greater",
                    "onnx.op.GreaterOrEqual",
                    "onnx.op.GridSample",
                    "onnx.op.GroupNormalization",
                    "onnx.op.HardSigmoid",
                    "onnx.op.Hardmax",
                    "onnx.op.HardSwish",
                    "onnx.op.Identity",
                    "onnx.op.If",
                    "onnx.op.InstanceNormalization",
                    "onnx.op.IsInf",
                    "onnx.op.IsNaN",
                    "onnx.op.LayerNormalization",
                    "onnx.op.LeakyReLU",
                    "onnx.op.Less",
                    "onnx.op.LessOrEqual",
                    "onnx.op.Log",
                    "onnx.op.LogSoftmax",
                    "onnx.op.Loop",
                    "onnx.op.LpNormalization",
                    "onnx.op.LpPool",
                    "onnx.op.MatMul",
                    "onnx.op.MatMulInteger",
                    "onnx.op.Max",
                    "onnx.op.MaxPool",
                    "onnx.op.MaxRoiPool",
                    "onnx.op.MaxUnpool",
                    "onnx.op.Mean",
                    "onnx.op.MeanVarianceNormalization",
                    "onnx.op.Min",
                    "onnx.op.Mish",
                    "onnx.op.Mod",
                    "onnx.op.Mul",
                    "onnx.op.Multinomial",
                    "onnx.op.Neg",
                    "onnx.op.NonMaxSuppression",
                    "onnx.op.NonZero",
                    "onnx.op.Not",
                    "onnx.op.OneHot",
                    "onnx.op.Optional",
                    "onnx.op.OptionalGetElement",
                    "onnx.op.OptionalHasElement",
                    "onnx.op.Or",
                    "onnx.op.Pad",
                    "onnx.op.Pow",
                    "onnx.op.PRelu",
                    "onnx.op.QLinearAdd",
                    "onnx.op.QLinearAveragePool",
                    "onnx.op.QLinearConcat",
                    "onnx.op.QLinearConv",
                    "onnx.op.QLinearMatMul",
                    "onnx.op.QLinearMul",
                    "onnx.op.QLinearSigmoid",
                    "onnx.op.QuantizeLinear",
                    "onnx.op.Range",
                    "onnx.op.ReduceL1",
                    "onnx.op.ReduceL2",
                    "onnx.op.ReduceLogSum",
                    "onnx.op.ReduceLogSumExp",
                    "onnx.op.ReduceMax",
                    "onnx.op.ReduceMean",
                    "onnx.op.ReduceMin",
                    "onnx.op.ReduceProd",
                    "onnx.op.ReduceSum",
                    "onnx.op.ReduceSumSquare",
                    "onnx.op.ReLU",
                    "onnx.op.Reshape",
                    "onnx.op.Resize",
                    "onnx.op.ReverseSequence",
                    "onnx.op.RNN",
                    "onnx.op.RoiAlign",
                    "onnx.op.Round",
                    "onnx.op.Scan",
                    "onnx.op.Scatter",
                    "onnx.op.ScatterElements",
                    "onnx.op.ScatterND",
                    "onnx.op.Selu",
                    "onnx.op.SequenceAt",
                    "onnx.op.SequenceConstruct",
                    "onnx.op.SequenceEmpty",
                    "onnx.op.SequenceErase",
                    "onnx.op.SequenceInsert",
                    "onnx.op.SequenceLength",
                    "onnx.op.Shrink",
                    "onnx.op.Sigmoid",
                    "onnx.op.Sign",
                    "onnx.op.Sin",
                    "onnx.op.Sinh",
                    "onnx.op.Size",
                    "onnx.op.Slice",
                    "onnx.op.Softmax",
                    "onnx.op.SoftmaxCrossEntropy",
                    "onnx.op.Softplus",
                    "onnx.op.Softsign",
                    "onnx.op.SpaceToDepth",
                    "onnx.op.Split",
                    "onnx.op.SplitToSequence",
                    "onnx.op.Sqrt",
                    "onnx.op.Squeeze",
                    "onnx.op.StringNormalizer",
                    "onnx.op.Sub",
                    "onnx.op.Sum",
                    "onnx.op.Tan",
                    "onnx.op.Tanh",
                    "onnx.op.TfIdfVectorizer",
                    "onnx.op.ThresholdedRelu",
                    "onnx.op.Tile",
                    "onnx.op.TopK",
                    "onnx.op.Transpose",
                    "onnx.op.Trilu",
                    "onnx.op.Unsqueeze",
                    "onnx.op.Upsample",
                    "onnx.op.Where",
                    "onnx.op.Xor",
                    "onnx.op.BERT",
                    "onnx.op.GPT2",
                    "onnx.op.ResNet50",
                    "onnx.op.MobileNet",
                    "onnx.op.EfficientNet",
                    "onnx.op.YOLO",
                    "onnx.op.SSD",
                    "onnx.op.FasterRCNN",
                    "onnx.op.MaskRCNN",
                    "onnx.op.DeepLab",
                    "onnx.op.UNet",
                ],
                "count": 175,
            },
            "tensorflow_savedmodel": {
                "description": "TensorFlow SavedModel and checkpoint metadata",
                "reference": "TensorFlow SavedModel Format Specification",
                "fields": [
                    "tf.saved_model.tags",
                    "tf.saved_model.graph_def.version",
                    "tf.saved_model.graph_def.library",
                    "tf.saved_model.graph_def.node_def.name",
                    "tf.saved_model.graph_def.node_def.op",
                    "tf.saved_model.graph_def.node_def.input",
                    "tf.saved_model.graph_def.node_def.attr",
                    "tf.saved_model.graph_def.function_def.name",
                    "tf.saved_model.graph_def.function_def.attr",
                    "tf.saved_model.graph_def.function_def.signature",
                    "tf.saved_model.meta_graph_def.version",
                    "tf.saved_model.meta_graph_def.tensor_array_ops",
                    "tf.saved_model.meta_graph_def.hash_table_ops",
                    "tf.saved_model.asset_file_def",
                    "tf.saved_model.asset_file_def.tensor_info",
                    "tf.saved_model.asset_file_def.filename",
                    "tf.checkpoint.variable_names",
                    "tf.checkpoint.variable_shapes",
                    "tf.checkpoint.variable_dtypes",
                    "tf.checkpoint.slice_spec",
                    "tf.checkpoint.num_shards",
                    "tf.keras.Model.name",
                    "tf.keras.Model.optimizer",
                    "tf.keras.Model.loss",
                    "tf.keras.Model.metrics",
                    "tf.keras.Model.epochs",
                    "tf.keras.Model.verbose",
                    "tf.keras.callbacks.History",
                    "tf.keras.layers.Layer.name",
                    "tf.keras.layers.Layer.trainable",
                    "tf.keras.layers.Layer.weights",
                    "tf.keras.layers.Dense.units",
                    "tf.keras.layers.Dense.activation",
                    "tf.keras.layers.Conv2D.filters",
                    "tf.keras.layers.Conv2D.kernel_size",
                    "tf.keras.layers.Conv2D.strides",
                    "tf.keras.layers.Conv2D.padding",
                    "tf.keras.layers.LSTM.units",
                    "tf.keras.layers.LSTM.return_sequences",
                    "tf.keras.layers.LSTM.return_state",
                    "tf.keras.layers.Dropout.rate",
                    "tf.keras.layers.BatchNormalization.momentum",
                    "tf.estimator.Estimator.config",
                    "tf.estimator.Estimator.model_dir",
                    "tf.estimator.Estimator.params",
                    "tf.estimator.Exporters",
                    "tf.feature_column",
                    "tf.train.Example.features",
                    "tf.train.Example.label",
                    "tf.train.FeatureList",
                    "tf.train.SequenceExample.context",
                    "tf.train.SequenceExample.feature_lists",
                    "tf.saved_traceback",
                    "tf.function.input_signature",
                    "tf.function.graph",
                    "tf.function.jit_compile",
                    "tf.data.Dataset",
                    "tf.data.Dataset.element_spec",
                ],
                "count": 60,
            },
            "pytorch_model": {
                "description": "PyTorch model and checkpoint metadata",
                "reference": "PyTorch Serialization Specification",
                "fields": [
                    "torch.version",
                    "torch.serialization.version",
                    "torch.serialization.protocol_version",
                    "torch.nn.Module.named_parameters",
                    "torch.nn.Module.named_buffers",
                    "torch.nn.Module.training",
                    "torch.nn.Module.eval",
                    "torch.nn.Linear.in_features",
                    "torch.nn.Linear.out_features",
                    "torch.nn.Conv2d.in_channels",
                    "torch.nn.Conv2d.out_channels",
                    "torch.nn.Conv2d.kernel_size",
                    "torch.nn.Conv2d.stride",
                    "torch.nn.Conv2d.padding",
                    "torch.nn.Conv2d.dilation",
                    "torch.nn.Conv2d.groups",
                    "torch.nn.LSTM.input_size",
                    "torch.nn.LSTM.hidden_size",
                    "torch.nn.LSTM.num_layers",
                    "torch.nn.LSTM.batch_first",
                    "torch.nn.LSTM.dropout",
                    "torch.nn.LSTM.bidirectional",
                    "torch.nn.BatchNorm2d.num_features",
                    "torch.nn.BatchNorm2d.eps",
                    "torch.nn.BatchNorm2d.momentum",
                    "torch.nn.Dropout.p",
                    "torch.nn.Embedding.num_embeddings",
                    "torch.nn.Embedding.embedding_dim",
                    "torch.nn.Embedding.padding_idx",
                    "torch.nn.Embedding.max_norm",
                    "torch.optim.Optimizer.defaults",
                    "torch.optim.Adam.lr",
                    "torch.optim.Adam.betas",
                    "torch.optim.Adam.eps",
                    "torch.optim.Adam.weight_decay",
                    "torch.optim.SGD.momentum",
                    "torch.optim.SGD.dampening",
                    "torch.optim.SGD.nesterov",
                    "torch.optim.lr_scheduler.StepLR.step_size",
                    "torch.optim.lr_scheduler.StepLR.gamma",
                    "torch.optim.lr_scheduler.ExponentialLR.gamma",
                    "torch.optim.lr_scheduler.CosineAnnealingLR.T_max",
                    "torch.optim.lr_scheduler.ReduceLROnPlateau.mode",
                    "torch.optim.lr_scheduler.ReduceLROnPlateau.factor",
                    "torch.optim.lr_scheduler.ReduceLROnPlateau.patience",
                    "torch.utils.data.Dataset",
                    "torch.utils.data.DataLoader.batch_size",
                    "torch.utils.data.DataLoader.shuffle",
                    "torch.utils.data.DataLoader.num_workers",
                    "torch.utils.data.DataLoader.pin_memory",
                    "torch.utils.data.DistributedSampler",
                    "torch.distributed.init_method",
                    "torch.distributed.rank",
                    "torch.distributed.world_size",
                    "torch.cuda.is_available",
                    "torch.cuda.device_count",
                    "torch.cuda.current_device",
                    "torch.cuda.get_device_name",
                    "torch.cuda.memory_allocated",
                    "torch.cuda.memory_reserved",
                    "torch.autograd.Function.forward",
                    "torch.autograd.Function.backward",
                    "torch.jit.script",
                    "torch.jit.trace",
                    "torch.jit.ScriptModule",
                    "torch.jit.ScriptFunction",
                    "torch.quantization.quantize_dynamic",
                    "torch.quantization.get_default_qconfig",
                    "torch.amp.autocast_mode",
                    "torch.cuda.amp.GradScaler",
                ],
                "count": 70,
            },
            "tensorrt_model": {
                "description": "NVIDIA TensorRT inference engine metadata",
                "reference": "TensorRT Developer Guide",
                "fields": [
                    "trt.version",
                    "trt.max_batch_size",
                    "trt.max_workspace_size",
                    "trt.fp16",
                    "trt.int8",
                    "trt.tf32",
                    "trt.builder_optimization_level",
                    "trt.tactic_sources",
                    "trt.algorithms",
                    "trt.engine.layers",
                    "trt.engine.num_layers",
                    "trt.engine.num_inputs",
                    "trt.engine.num_outputs",
                    "trt.engine.device_memory_size",
                    "trt.engine.max_batch_size",
                    "trt.engine.max_workspace_size",
                    "trt.engine.memory_pool_type",
                    "trt.engine.dla_core",
                    "trt.engine.dla_load",
                    "trt.engine.dla_enable",
                    "trt.engine.num_DLA_layers",
                    "trt.engine.stream",
                    "trt.execution_context",
                    "trt.context.debug_sync",
                    "trt.context.enqueue",
                    "trt.context.execute",
                    "trt.binding.name",
                    "trt.binding.shape",
                    "trt.binding.dtype",
                    "trt.binding.is_input",
                    "trt.binding.allocate",
                    "trt.profiling_verbosity",
                    "trt.profiling_verbosity.layer_names_only",
                    "trt.profiling_verbosity.detailed",
                    "trt.calibration_table",
                    "trt.int8_calibrator",
                    "trt.int8_calibrator.get_batch_size",
                    "trt.int8_calibrator.get_batch",
                    "trt.int8_calibrator.get_algorithm",
                    "trt.parser.plugin_factory",
                    "trt.onnx_parser.flags",
                    "trt.uff_parser.plugin_registry",
                    "trt.caffe_parser.protobuf",
                    "trt.tensorrt_version",
                    "trt.cuda_engine_get_context_binding_size",
                    "trt.cuda_engine_get_tensor_location",
                    "trt.cuda_engine_is_shape_binding",
                    "trt.cuda_engine_get_tensor_format",
                    "trt.cuda_engine_get_tensor_format_desc",
                    "trt.cuda_engine_destroy",
                    "trt.logger.severity",
                    "trt.logger.log_level",
                    "trt.precision_flag",
                    "trt.reformat_free",
                    "trt.tensor_format",
                    "trt.performance_query",
                    "trt.performance_counter",
                    "trt.latency_traker",
                    "trt.timing_cache",
                    "trt.timing_cache_merge",
                    "trt.cluster_builder",
                    "trt.cluster_builder.get_cluster",
                    "trt.cluster_builder.add_strategy",
                    "trt.cluster_builder.set_algorithm",
                    "trt.cluster_builder.clear",
                ],
                "count": 65,
            },
            "openvino_model": {
                "description": "Intel OpenVINO model metadata",
                "reference": "OpenVINO Toolkit Documentation",
                "fields": [
                    "openvino.version",
                    "openvino.ir_version",
                    "openvino.model.name",
                    "openvino.model.inputs",
                    "openvino.model.outputs",
                    "openvino.model.parameters",
                    "openvino.model.results",
                    "openvino.model.opsets",
                    "openvino.opset.version",
                    "openvino.opset.op_type",
                    "openvino.input.name",
                    "openvino.input.shape",
                    "openvino.input.element_type",
                    "openvino.output.name",
                    "openvino.output.shape",
                    "openvino.output.element_type",
                    "openvino.precision.fp32",
                    "openvino.precision.fp16",
                    "openvino.precision.int8",
                    "openvino.precision.u8",
                    "openvino.precision.i8",
                    "openvino.precision.i32",
                    "openvino.precision.i64",
                    "openvino.precision.bool",
                    "openvino.layout.NCHW",
                    "openvino.layout.NHWC",
                    "openvino.layout.HWCN",
                    "openvino.layout.CHW",
                    "openvino.layout.HWC",
                    "openvino.layout.blocked",
                    "openvino.affine_output",
                    "openvino.fake_quantize",
                    "openvino.region_to_output",
                    "openvino.const_output",
                    "openvino.executable_network",
                    "openvino.infer_request",
                    "openvino.infer_request.infer",
                    "openvino.infer_request.get_tensor",
                    "openvino.infer_request.set_tensor",
                    "openvino.async_infer",
                    "openvino.compiled_model",
                    "openvino.compiled_model.inputs",
                    "openvino.compiled_model.outputs",
                    "openvino.compiled_model.profiling_info",
                    "openvino.compiled_model.export",
                    "openvino.read_network",
                    "openvino.serialize",
                    "openvino.preprocess",
                    "openvino.preprocess.input",
                    "openvino.preprocess.output",
                    "openvino.preprocess.resize_mode",
                    "openvino.preprocess.mean",
                    "openvino.preprocess.scale",
                    "openvino.preprocess.convert_element_type",
                    "openvino.preprocess.convert_layout",
                    "openvino.preprocess.tensor_color_space",
                    "openvino.postprocess",
                    "openvino.postprocess.output_precision",
                    "openvino.postprocess.output_layout",
                    "openvino.ovc_convert_model",
                    "openvino.mo_convert_model",
                    "openvino.mo.front",
                    "openvino.mo.back",
                    "openvino.mo.layout",
                    "openvino.mo.quantize",
                    "openvino.mo.compress_to_fp16",
                    "openvino.auto_config",
                    "openvino.auto_config.device",
                    "openvino.auto_config.accelerator",
                    "openvino.hetero_config",
                    "openvino.multi_config",
                    "openvino.cache_dir",
                    "openvino.performance_mode",
                    "openvino.performance_mode.TUNING",
                    "openvino.performance_mode.HINT",
                    "openvino.execution_mode",
                    "openvino.execution_mode.HETERO",
                ],
                "count": 75,
            },
            "training_metadata": {
                "description": "Model training metadata and hyperparameters",
                "reference": "ML Training Best Practices",
                "fields": [
                    "training.epochs",
                    "training.batch_size",
                    "training.learning_rate",
                    "training.learning_rate_schedule",
                    "training.optimizer",
                    "training.optimizer.momentum",
                    "training.optimizer.beta1",
                    "training.optimizer.beta2",
                    "training.optimizer.epsilon",
                    "training.optimizer.weight_decay",
                    "training.loss_function",
                    "training.loss_function.cross_entropy",
                    "training.loss_function.mse",
                    "training.loss_function.mae",
                    "training.loss_function.smooth_mae",
                    "training.loss_function.contrastive",
                    "training.metrics.accuracy",
                    "training.metrics.precision",
                    "training.metrics.recall",
                    "training.metrics.f1_score",
                    "training.metrics.auc",
                    "training.metrics.mAP",
                    "training.metrics.iou",
                    "training.metrics.dice",
                    "training.metrics.bleu",
                    "training.metrics.rouge",
                    "training.metrics.perplexity",
                    "training.early_stopping.patience",
                    "training.early_stopping.min_delta",
                    "training.early_stopping.restore_best_weights",
                    "training.checkpoint.save_freq",
                    "training.checkpoint.save_best_only",
                    "training.checkpoint.monitor",
                    "training.checkpoint.mode",
                    "training.checkpoint.verbose",
                    "training.checkpoint.filepath",
                    "training.validation_split",
                    "training.test_split",
                    "training.shuffle_buffer_size",
                    "training.prefetch_size",
                    "training.num_parallel_calls",
                    "training.cache",
                    "training.repeat",
                    "training.seed",
                    "training.global_batch_norm",
                    "training.gradient_accumulation_steps",
                    "training.gradient_clipping",
                    "training.gradient_clipping_value",
                    "training.mixed_precision",
                    "training.mixed_precision.scale_factor",
                    "training.distributed_training",
                    "training.distributed_backend",
                    "training.distributed.world_size",
                    "training.distributed.rank",
                    "training.distributed.local_rank",
                    "training.distributed.sync_bn",
                    "training.distributed.find_unused_parameters",
                    "training.data_augmentation.flip",
                    "training.data_augmentation.rotation",
                    "training.data_augmentation.crop",
                    "training.data_augmentation.zoom",
                    "training.data_augmentation.color_jitter",
                    "training.data_augmentation.gaussian_noise",
                    "training.data_augmentation.mixup",
                    "training.data_augmentation.cutmix",
                    "training.data_augmentation.rand_augment",
                    "training.hpo.search_algorithm",
                    "training.hpo.search_space",
                    "training.hpo.num_trials",
                    "training.hpo.max_epochs",
                    "training.hpo.objective",
                    "training.hpo.pruning",
                    "training.hpo.pruning_interval",
                ],
                "count": 85,
            },
            "inference_metadata": {
                "description": "Model inference and deployment metadata",
                "reference": "ML Inference Best Practices",
                "fields": [
                    "inference.batch_size",
                    "inference.max_batch_size",
                    "inference.timeout",
                    "inference.max_concurrent_requests",
                    "inference.preprocessing",
                    "inference.postprocessing",
                    "inference.confidence_threshold",
                    "inference.iou_threshold",
                    "inference.max_detections",
                    "inference.return_embeddings",
                    "inference.return_attention",
                    "inference.top_k",
                    "inference.num_classes",
                    "inference.class_names",
                    "inference.label_map",
                    "inference.anchors",
                    "inference.mask_threshold",
                    "inference.nms_threshold",
                    "inference.soft_nms_sigma",
                    "inference.class_agnostic_nms",
                    "inference.filter_empty_detections",
                    "inference.keep_top_k",
                    "inference.score_threshold",
                    "inference.use_gpu",
                    "inference.gpu_memory_fraction",
                    "inference.gpu_device_id",
                    "inference.tensorrt_precision",
                    "inference.onnx_provider",
                    "inference.openvino_device",
                    "inference.coreml_flags",
                    "inference.tflite_options",
                    "inference.quantization_type",
                    "inference.quantization.calibration",
                    "inference.quantization.symmetric",
                    "inference.quantization.asymmetric",
                    "inference.quantization.per_channel",
                    "inference.quantization.activations",
                    "inference.latency_p50",
                    "inference.latency_p90",
                    "inference.latency_p99",
                    "inference.throughput",
                    "inference.model_version",
                    "inference.api_version",
                    "inference.deployment_region",
                    "inference.endpoint",
                    "inference.serving_framework",
                    "inference.serving_framework.tensorflow_serving",
                    "inference.serving_framework.torchserve",
                    "inference.serving_framework.triton",
                    "inference.serving_framework.tfx",
                    "inference.serving_framework.kserve",
                    "inference.serving_framework.seldon",
                    "inference.monitoring.metrics",
                    "inference.monitoring.latency",
                    "inference.monitoring.throughput",
                    "inference.monitoring.error_rate",
                    "inference.monitoring.gpu_utilization",
                    "inference.monitoring.memory_usage",
                    "inference.monitoring.model_load_time",
                    "inference.a_b_testing",
                    "inference.canary_deployment",
                    "inference.rollback_threshold",
                ],
                "count": 75,
            },
            "data_pipeline": {
                "description": "Data pipeline and preprocessing metadata",
                "reference": "Data Pipeline Standards",
                "fields": [
                    "pipeline.source.type",
                    "pipeline.source.path",
                    "pipeline.source.format",
                    "pipeline.source.shuffle",
                    "pipeline.source.cache",
                    "pipeline.source.num_workers",
                    "pipeline.source.prefetch",
                    "pipeline.transformations.normalize",
                    "pipeline.transformations.standardize",
                    "pipeline.transformations.resize",
                    "pipeline.transformations.crop",
                    "pipeline.transformations.flip",
                    "pipeline.transformations.rotate",
                    "pipeline.transformations.color_jitter",
                    "pipeline.transformations.affine",
                    "pipeline.transformations.elastic_transform",
                    "pipeline.transformations.augmentation",
                    "pipeline.transformations.compose",
                    "pipeline.transformations.lambda",
                    "pipeline.transformations.random_apply",
                    "pipeline.transformations.to_tensor",
                    "pipeline.transformations.pil_to_tensor",
                    "pipeline.transformations.convert_image_dtype",
                    "pipeline.transformations.pad",
                    "pipeline.transformations.random_crop",
                    "pipeline.transformations.random_resized_crop",
                    "pipeline.transformations.center_crop",
                    "pipeline.transformations.five_crop",
                    "pipeline.transformations.ten_crop",
                    "pipeline.transformations.grayscale",
                    "pipeline.transformations.random_grayscale",
                    "pipeline.transformations.random_perspective",
                    "pipeline.transformations.random_affine",
                    "pipeline.transformations.random_elastic",
                    "pipeline.transformations.autoaugment",
                    "pipeline.transformations.quantize",
                    "pipeline.transformations.dequantize",
                    "pipeline.batching.batch_size",
                    "pipeline.batching.drop_last",
                    "pipeline.batching.collate_fn",
                    "pipeline.batching.padding",
                    "pipeline.batching.padding_value",
                    "pipeline.batching.max_length",
                    "pipeline.batching.truncation",
                    "pipeline.split.train_ratio",
                    "pipeline.split.val_ratio",
                    "pipeline.split.test_ratio",
                    "pipeline.split.stratify",
                    "pipeline.split.shuffle",
                    "pipeline.split.random_state",
                    "pipeline.split.num_folds",
                    "pipeline.split.kfold",
                    "pipeline.split.stratified_kfold",
                    "pipeline.labeling.label_map",
                    "pipeline.labeling.class_weights",
                    "pipeline.labeling.oversampling",
                    "pipeline.labeling.undersampling",
                    "pipeline.labeling.smote",
                    "pipeline.labeling.label_smoothing",
                    "pipeline.streaming.chunk_size",
                    "pipeline.streaming.buffer_size",
                    "pipeline.streaming.num_prefetch",
                    "pipeline.streaming.repeat",
                    "pipeline.streaming.shuffle_buffer",
                    "pipeline.streaming.interleave",
                    "pipeline.streaming.parallel_interleave",
                    "pipeline.streaming.prefetch_map_function",
                    "pipeline.versioning.dataset_version",
                    "pipeline.versioning.schema_version",
                    "pipeline.profiling.num_samples",
                    "pipeline.profiling.column_types",
                    "pipeline.profiling.statistics",
                ],
                "count": 95,
            },
        },
        "totals": {
            "categories": 8,
            "total_fields": 742,
        }
    }

    return INVENTORY


def main():
    print("=" * 70)
    print("MACHINE LEARNING AND AI FORMAT METADATA FIELD INVENTORY")
    print("=" * 70)
    print()

    inventory = generate_ml_ai_inventory()

    print(f"Generated: {inventory['generated_at']}")
    print(f"Categories: {inventory['totals']['categories']}")
    print(f"Total Fields: {inventory['totals']['total_fields']}")
    print()

    print("FIELD COUNTS BY CATEGORY:")
    print("-" * 50)
    for cat_name, cat_data in sorted(inventory['categories'].items(), key=lambda x: x[1]['count'], reverse=True):
        print(f"  {cat_name:30s}: {cat_data['count']:>4} fields")

    output_dir = Path("dist/ml_ai_inventory")
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / "ml_ai_inventory.json"
    output_file.write_text(json.dumps(inventory, indent=2, sort_keys=True), encoding="utf-8")
    print()
    print(f"Wrote: {output_file}")

    summary = {
        "generated_at": inventory["generated_at"],
        "source": inventory["source"],
        "description": inventory["description"],
        "categories": {
            k: {
                "count": v["count"],
                "description": v["description"],
                "reference": v.get("reference", "N/A")
            }
            for k, v in inventory["categories"].items()
        },
        "totals": inventory["totals"],
    }

    summary_file = output_dir / "ml_ai_summary.json"
    summary_file.write_text(json.dumps(summary, indent=2, sort_keys=True), encoding="utf-8")
    print(f"Wrote: {summary_file}")


if __name__ == "__main__":
    main()
